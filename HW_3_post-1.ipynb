{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "Generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost. In this assignment, we'll use what we learned in Preprocessing module to compare ChatGPT-generated reviews with human-generated reviews. \n",
    "- A dataset with 300 reviews has been provided for you to use.\n",
    "- Label: Binary label indicating the class (0=authentic, 1=machine-generated).\n",
    "- The dataset can be found in this paper: https://arxiv.org/abs/2401.08825.\n",
    "\n",
    "Hint: you may find it is convenient to use `Spacy` package for this assignment. Outputs displayed here are ONLY for your refereces! You may get slightly different outputs if you use different packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is delicious and extremely flavorful....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, so I didn't expect much in terms of Kore...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently visited the new Veggie Grill in NYC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm a fan of 5Napkin Burger for 2 distinct tim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe's Pizza offers the finest slice in Times S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  The food is delicious and extremely flavorful....      0\n",
       "1  Okay, so I didn't expect much in terms of Kore...      0\n",
       "2  I recently visited the new Veggie Grill in NYC...      1\n",
       "3  I'm a fan of 5Napkin Burger for 2 distinct tim...      0\n",
       "4  Joe's Pizza offers the finest slice in Times S...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"review.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Tokenize function `(2 points)`\n",
    "\n",
    "Define a function `tokenize(doc, lemmatized = False, remove_stopword = False, remove_punct = True, pos_tag = False)`  as follows:\n",
    "   - Take five parameters: \n",
    "       - `doc`: a document (e.g., a review)\n",
    "       - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is False (i.e., tokens are not lemmatized).\n",
    "       - `remove_stopword`: an optional boolean parameter to remove stop words. The default value is False (i.e., do not remove stop words). \n",
    "       - `remove_punct`: an optional boolean parameter to remove punctuations. The default value is True (i.e., remove punctuations).\n",
    "      - `pos_tag`: whether the POS tag of each token is returned. Please use the universal POS tag. The default is False (i.e., do not return POS tag).\n",
    "   - Split the input document into unigrams and also clean up tokens as follows:\n",
    "       - if `lemmatized` is turned on, lemmatize all unigrams. \n",
    "       - if `remove_stopword` is set to True, remove all stop words. \n",
    "       - if `remove_punct` is set to True, remove all punctuation tokens. \n",
    "       - remove all empty tokens and lowercase all the tokens.\n",
    "       - if `pos_tag = True`, retrieve the POS tag for each of the resulting tokens and make a tuple (token, pos_tag) for the token.\n",
    "   - Return the list of tokens (including POS tag if the option is on) obtained for the document after all the processing. \n",
    "   \n",
    "(Hint: you can use spacy package for this task. For reference, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc, lemmatized=False, remove_stopword=False, remove_punct = True, pos_tag = False):\n",
    "    doc = nlp(doc.lower()) \n",
    "    tokens = [token for token in doc if not token.is_space]\n",
    "    \n",
    "    if remove_punct:\n",
    "        tokens = [token.text for token in tokens if not token.is_punct]\n",
    "        \n",
    "    if remove_stopword:\n",
    "        spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        if type(tokens[0]) == str:\n",
    "            tokens = [token for token in tokens if token not in spacy_stopwords]\n",
    "        else:\n",
    "            tokens = [token.text for token in tokens if token.text not in spacy_stopwords]\n",
    "\n",
    "    if lemmatized:\n",
    "        if type(tokens[0]) != spacy.tokens.token.Token:\n",
    "            tokens = [nlp(token)[0] for token in tokens]\n",
    "        tokens = [token.lemma_ for token in tokens]\n",
    "        if remove_stopword:\n",
    "            tokens = [token for token in tokens if token not in spacy_stopwords]\n",
    "    \n",
    "    if pos_tag:\n",
    "        if type(tokens[0]) != spacy.tokens.token.Token:\n",
    "            tokens = [nlp(token)[0] for token in tokens]\n",
    "        tokens = [(token.text, token.pos_) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function with different parameter configuration and observe the differences in the resulting tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recently visited the new Veggie Grill in NYC and it was an absolute delight! The Nashville hot 'chicken' blew me away with its spot-on flavors and textures – it's a must-try. The vegan buffalo 'wings' were the highlight for me; truly the best I've ever had, hands down. The Mac and cheese also did not disappoint, rounding out a meal that has me eager to return. Anyone on the fence about Veggie Grill needs to take the plunge. Get ready to be wowed by a vegan menu that hits all the right notes. I'm already planning my next visit to conquer the rest of the menu. This place is a solid 10/10.\n",
      "\n",
      "1.lemmatized=False, remove_stopword=False, remove_punct = True,  pos_tag = False:\n",
      " ['the', 'food', 'is', 'delicious', 'and', 'extremely', 'flavorful', 'the', 'portion', 'sizes', 'are', 'huge', 'you', 'wo', \"n't\", 'leave', 'hungry', 'i', 'ordered', 'chicken', 'and', 'waffles', 'cornbread', 'is', 'served', 'here', 'for', 'free', 'i', 'went', 'with', 'my', 'sister', 'and', 'she', 'got', 'meatloaf', 'and', 'collard', 'greens', 'with', 'cole', 'slaw', 'the', 'collard', 'greens', 'were', 'one', 'of', 'my', 'favorites', 'the', 'coleslaw', 'was', 'much', 'different', 'then', 'the', 'cheap', 'coleslaw', 'you', \"'ll\", 'buy', 'at', 'kfc', 'or', 'the', 'grocery', 'stores', 'the', 'vegetables', 'are', 'thick', 'it', 'was', 'interesting', 'and', 'really', 'worked', 'for', 'me', 'i', 'was', 'too', 'full', 'off', 'of', 'my', 'chicken', 'and', 'waffle', 'to', 'taste', 'her', 'meatloaf', 'the', 'only', 'downside', 'to', 'this', 'place', 'i', 'think', 'is', 'cleanliness', 'they', 'need', 'to', 'put', 'more', 'cars', 'into', 'cleaning', 'the', 'area', 'before', 'seating', 'customers', 'it', 'was', 'a', 'little', 'gross', 'around', 'me', 'that', \"'s\", 'what', 'stopped', 'me', 'from', 'giving', 'this', 'place', '5', 'stars']\n",
      "\n",
      "2.lemmatized=False, remove_stopword=False, remove_punct = True,  pos_tag = True:\n",
      " [('the', 'PRON'), ('food', 'NOUN'), ('is', 'AUX'), ('delicious', 'PROPN'), ('and', 'CCONJ'), ('extremely', 'ADV'), ('flavorful', 'ADJ'), ('the', 'PRON'), ('portion', 'NOUN'), ('sizes', 'VERB'), ('are', 'AUX'), ('huge', 'ADJ'), ('you', 'PRON'), ('wo', 'AUX'), (\"n't\", 'PART'), ('leave', 'VERB'), ('hungry', 'ADJ'), ('i', 'PRON'), ('ordered', 'VERB'), ('chicken', 'NOUN'), ('and', 'CCONJ'), ('waffles', 'NOUN'), ('cornbread', 'NOUN'), ('is', 'AUX'), ('served', 'VERB'), ('here', 'ADV'), ('for', 'ADP'), ('free', 'ADJ'), ('i', 'PRON'), ('went', 'VERB'), ('with', 'ADP'), ('my', 'PRON'), ('sister', 'NOUN'), ('and', 'CCONJ'), ('she', 'PRON'), ('got', 'VERB'), ('meatloaf', 'NOUN'), ('and', 'CCONJ'), ('collard', 'NOUN'), ('greens', 'NOUN'), ('with', 'ADP'), ('cole', 'PROPN'), ('slaw', 'NOUN'), ('the', 'PRON'), ('collard', 'NOUN'), ('greens', 'NOUN'), ('were', 'AUX'), ('one', 'NUM'), ('of', 'ADP'), ('my', 'PRON'), ('favorites', 'NOUN'), ('the', 'PRON'), ('coleslaw', 'NOUN'), ('was', 'AUX'), ('much', 'ADJ'), ('different', 'ADJ'), ('then', 'ADV'), ('the', 'PRON'), ('cheap', 'ADJ'), ('coleslaw', 'NOUN'), ('you', 'PRON'), (\"'ll\", 'AUX'), ('buy', 'VERB'), ('at', 'ADP'), ('kfc', 'NOUN'), ('or', 'CCONJ'), ('the', 'PRON'), ('grocery', 'NOUN'), ('stores', 'NOUN'), ('the', 'PRON'), ('vegetables', 'NOUN'), ('are', 'AUX'), ('thick', 'ADJ'), ('it', 'PRON'), ('was', 'AUX'), ('interesting', 'ADJ'), ('and', 'CCONJ'), ('really', 'ADV'), ('worked', 'VERB'), ('for', 'ADP'), ('me', 'PRON'), ('i', 'PRON'), ('was', 'AUX'), ('too', 'ADV'), ('full', 'ADJ'), ('off', 'ADP'), ('of', 'ADP'), ('my', 'PRON'), ('chicken', 'NOUN'), ('and', 'CCONJ'), ('waffle', 'NOUN'), ('to', 'PART'), ('taste', 'NOUN'), ('her', 'PRON'), ('meatloaf', 'NOUN'), ('the', 'PRON'), ('only', 'ADV'), ('downside', 'ADV'), ('to', 'PART'), ('this', 'PRON'), ('place', 'NOUN'), ('i', 'PRON'), ('think', 'VERB'), ('is', 'AUX'), ('cleanliness', 'NOUN'), ('they', 'PRON'), ('need', 'VERB'), ('to', 'PART'), ('put', 'VERB'), ('more', 'ADV'), ('cars', 'NOUN'), ('into', 'ADP'), ('cleaning', 'VERB'), ('the', 'PRON'), ('area', 'NOUN'), ('before', 'ADP'), ('seating', 'VERB'), ('customers', 'NOUN'), ('it', 'PRON'), ('was', 'AUX'), ('a', 'PRON'), ('little', 'ADJ'), ('gross', 'ADJ'), ('around', 'ADV'), ('me', 'PRON'), ('that', 'SCONJ'), (\"'s\", 'PART'), ('what', 'PRON'), ('stopped', 'VERB'), ('me', 'PRON'), ('from', 'ADP'), ('giving', 'VERB'), ('this', 'PRON'), ('place', 'NOUN'), ('5', 'NUM'), ('stars', 'NOUN')]\n",
      "\n",
      "3.lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = False:\n",
      " ['food', 'delicious', 'extremely', 'flavorful', 'portion', 'size', 'huge', 'leave', 'hungry', 'order', 'chicken', 'waffle', 'cornbread', 'serve', 'free', 'sister', 'meatloaf', 'collard', 'green', 'cole', 'slaw', 'collard', 'green', 'favorite', 'coleslaw', 'different', 'cheap', 'coleslaw', 'buy', 'kfc', 'grocery', 'store', 'vegetable', 'thick', 'interesting', 'work', 'chicken', 'waffle', 'taste', 'meatloaf', 'downside', 'place', 'think', 'cleanliness', 'need', 'car', 'clean', 'area', 'seat', 'customer', 'little', 'gross', 'stop', 'place', '5', 'star']\n",
      "\n",
      "4.lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = True:\n",
      " [('food', 'NOUN'), ('delicious', 'PROPN'), ('extremely', 'ADV'), ('flavorful', 'ADJ'), ('portion', 'NOUN'), ('size', 'NOUN'), ('huge', 'ADJ'), ('leave', 'VERB'), ('hungry', 'ADJ'), ('order', 'NOUN'), ('chicken', 'NOUN'), ('waffle', 'NOUN'), ('cornbread', 'NOUN'), ('serve', 'VERB'), ('free', 'ADJ'), ('sister', 'NOUN'), ('meatloaf', 'NOUN'), ('collard', 'NOUN'), ('green', 'PROPN'), ('cole', 'PROPN'), ('slaw', 'NOUN'), ('collard', 'NOUN'), ('green', 'PROPN'), ('favorite', 'ADJ'), ('coleslaw', 'NOUN'), ('different', 'ADJ'), ('cheap', 'ADJ'), ('coleslaw', 'NOUN'), ('buy', 'VERB'), ('kfc', 'NOUN'), ('grocery', 'NOUN'), ('store', 'NOUN'), ('vegetable', 'NOUN'), ('thick', 'ADJ'), ('interesting', 'ADJ'), ('work', 'VERB'), ('chicken', 'NOUN'), ('waffle', 'NOUN'), ('taste', 'NOUN'), ('meatloaf', 'NOUN'), ('downside', 'ADV'), ('place', 'NOUN'), ('think', 'VERB'), ('cleanliness', 'NOUN'), ('need', 'VERB'), ('car', 'NOUN'), ('clean', 'ADJ'), ('area', 'NOUN'), ('seat', 'NOUN'), ('customer', 'NOUN'), ('little', 'ADJ'), ('gross', 'ADJ'), ('stop', 'VERB'), ('place', 'NOUN'), ('5', 'NUM'), ('star', 'PROPN')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, We will test one document generated by GPT\n",
    "\n",
    "print(data[\"text\"].iloc[2] + \"\\n\")\n",
    "\n",
    "print(f\"1.lemmatized=False, remove_stopword=False, remove_punct = True,  pos_tag = False:\\n \\\n",
    "{tokenize(data['text'].iloc[0], lemmatized=False, remove_stopword=False, remove_punct = True, pos_tag = False)}\\n\")\n",
    "\n",
    "print(f\"2.lemmatized=False, remove_stopword=False, remove_punct = True,  pos_tag = True:\\n \\\n",
    "{tokenize(data['text'].iloc[0], lemmatized=False, remove_stopword=False, remove_punct = True, pos_tag = True)}\\n\")\n",
    "\n",
    "print(f\"3.lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = False:\\n \\\n",
    "{tokenize(data['text'].iloc[0], lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = False)}\\n\")\n",
    "\n",
    "print(f\"4.lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = True:\\n \\\n",
    "{tokenize(data['text'].iloc[0], lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = True)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Quantify concreteness (3 points)\n",
    "\n",
    "\n",
    "`Concreteness` can increase a message's persuasion. The concreteness can be measured by the use of :\n",
    "- `article (DET)` (e.g., a, an, and the), \n",
    "- `adpositions (ADP)` (e.g., in, at, of, on, etc), and\n",
    "- `adjectives(ADJ)` before `nouns(NN)`, i.e., a bigram where the first word is adjective and the second one is noun.\n",
    "\n",
    "Note: corresponding POS tags have been provided in the parentheses above.\n",
    "\n",
    "Define a function `compute_concreteness(doc)` as follows:\n",
    "- Input argument is a document, i.e., `doc`\n",
    "- Call your function defined in Q1 with `lemmatized=False, remove_stopword=False, remove_punct = False, pos_tag = True` to generate a list of tokens with POS tags for the input document.\n",
    "- Generate bigrams out of the token list\n",
    "- Find unigrams with tags `article` or `adposition` \n",
    "- Find bigrams where the first word is `adjective` and the second is `noun`.\n",
    "- Compute `concereness` score as:  `((the counts of unigrams found) +  2 * (the number of bigrams found))/(total non-punctuation tokens)`.\n",
    "- return the concreteness score, the list of article tokens, the list of adposition tokens, and the list of bigrams.\n",
    "- Do you think, overall, ChatGPT-generated answers are more concrete than human answers? Test your hypothesis statistically (e.g., t-test) and explain your finding in text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concreteness(doc):\n",
    "    tokens = tokenize(doc, lemmatized=False, remove_stopword=False, remove_punct=False, pos_tag=True)\n",
    "    #find unigrams with tags article or adposition\n",
    "    articles = [(token, tag) for (token, tag) in tokens if tag=='DET']\n",
    "    adpositions = [(token, tag) for (token, tag) in tokens if tag=='ADP']\n",
    "    #find bigrams with first word adjective and second word noun\n",
    "    bigrams=list(nltk.bigrams(tokens))\n",
    "    phrases=[(x[0],y[0]) for (x,y) in bigrams if x[1]==('ADJ') and y[1]==('NOUN')]\n",
    "    #compute concreteness score ((counts of unigrams) + 2 * (number of bigrams)) / (total non-punct tokens)\n",
    "    nonPunct = [token for (token, tag) in tokens if tag != 'PUNCT']\n",
    "    concreteness = ((len(articles)+len(adpositions))+2*(len(phrases)))/ (len(nonPunct))\n",
    "    # return concreteness score, list of article tokens, list of adposition tokens, list of bigrams\n",
    "    return (concreteness, articles, adpositions, phrases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: I recently visited the new Veggie Grill in NYC and it was an absolute delight! The Nashville hot 'chicken' blew me away with its spot-on flavors and textures – it's a must-try. The vegan buffalo 'wings' were the highlight for me; truly the best I've ever had, hands down. The Mac and cheese also did not disappoint, rounding out a meal that has me eager to return. Anyone on the fence about Veggie Grill needs to take the plunge. Get ready to be wowed by a vegan menu that hits all the right notes. I'm already planning my next visit to conquer the rest of the menu. This place is a solid 10/10. \n",
      "\n",
      "Concreteness: 0.3030 \n",
      "\n",
      "Articles:  [('this', 'DET'), ('the', 'DET'), ('the', 'DET'), ('the', 'DET'), ('the', 'DET'), ('the', 'DET'), ('the', 'DET'), ('the', 'DET'), ('all', 'DET'), ('the', 'DET'), ('a', 'DET'), ('a', 'DET'), ('any', 'DET'), ('the', 'DET'), ('a', 'DET'), ('the', 'DET'), ('a', 'DET'), ('the', 'DET'), ('the', 'DET'), ('the', 'DET'), ('the', 'DET'), ('a', 'DET'), ('no', 'DET'), ('the', 'DET'), ('a', 'DET'), ('some', 'DET'), ('the', 'DET')] \n",
      "\n",
      "Adpositions: [('in', 'ADP'), ('of', 'ADP'), ('in', 'ADP'), ('on', 'ADP'), ('at', 'ADP'), ('of', 'ADP'), ('for', 'ADP'), ('of', 'ADP'), ('in', 'ADP'), ('with', 'ADP'), ('as', 'ADP'), ('with', 'ADP'), ('of', 'ADP'), ('at', 'ADP'), ('like', 'ADP'), ('in', 'ADP'), ('behind', 'ADP'), ('outside', 'ADP'), ('of', 'ADP')] \n",
      "\n",
      "(ADJ, NOUNS): [('korean', 'food'), ('lighter', 'side'), ('right', 'ways'), ('fried', 'rice'), ('complimentary', 'broth'), ('palate', 'cleanser'), ('free', 'side'), ('real', 'dishes'), ('other', 'locations'), ('same', 'owner'), ('korean', 'food'), ('eastern', 'queens')]\n"
     ]
    }
   ],
   "source": [
    "concreteness, articles, adpositions, quantifier = compute_concreteness(data[\"text\"].iloc[1])\n",
    "print(f\"Question: {data['text'].iloc[2]} \\n\\nConcreteness: {concreteness :.4f} \\n\\nArticles:  {articles} \\n\\nAdpositions: {adpositions} \\n\\n(ADJ, NOUNS): {quantifier}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['concrete'] = data['text'].apply(lambda x: compute_concreteness(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>concrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is delicious and extremely flavorful....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, so I didn't expect much in terms of Kore...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently visited the new Veggie Grill in NYC...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm a fan of 5Napkin Burger for 2 distinct tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.284091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe's Pizza offers the finest slice in Times S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.326087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  concrete\n",
       "0  The food is delicious and extremely flavorful....      0  0.227941\n",
       "1  Okay, so I didn't expect much in terms of Kore...      0  0.303030\n",
       "2  I recently visited the new Veggie Grill in NYC...      1  0.307692\n",
       "3  I'm a fan of 5Napkin Burger for 2 distinct tim...      0  0.284091\n",
       "4  Joe's Pizza offers the finest slice in Times S...      1  0.326087"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic:  14.446783496031298   p-value:  3.192650740626651e-36\n",
      "The concreteness score of ChatGPT generated text is on average greater than the concreteness score of human generated text, to a statistically significant degree.\n",
      "We can say that LLM-generated text is more concrete than human reviews.\n"
     ]
    }
   ],
   "source": [
    "# Is LLM-generated review more concrete than human review? What is your finding\n",
    "\n",
    "from scipy import stats\n",
    "human = data[data['label']==0]['concrete']\n",
    "gpt = data[data['label']==1]['concrete']\n",
    "\n",
    "(statistic, pValue) = stats.ttest_ind(gpt, human)\n",
    "print (\"t-statistic: \", statistic, \"  p-value: \", pValue)\n",
    "if (pValue<0.01):\n",
    "    print(\"The concreteness score of ChatGPT generated text is on average greater than the concreteness score of human generated text, to a statistically significant degree.\\nWe can say that LLM-generated text is more concrete than human reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Sentiment Analysis (3 points)\n",
    "\n",
    "\n",
    "Let's check if there is any difference in sentiment between ChatGPT-generated and human-generated text.\n",
    "\n",
    "\n",
    "Define a function `compute_sentiment(tokenized_docs, pos, neg )` as follows:\n",
    "- take three parameters:\n",
    "    - `tokenized_docs` is the tokenized reviews by the `tokenize` function in Q1.\n",
    "    - `pos` (`neg`) is the lists of positive (negative) words, which can be find in Canvas Preprocessing module.\n",
    "- for each doc, compute the sentiment as `(#pos - #neg )/(#pos + #neg)`, where `#pos`(`#neg`) is the number of positive (negative) words. If a doc contains none of the positive or negative words, set the sentiment to 0.\n",
    "- return the sentiment column \"sentiment\" of DataFrame.\n",
    "\n",
    "\n",
    "Analysis: \n",
    "- Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how sentiment results change.\n",
    "- Do you think, in general, which tokenization configuration should be used? Why does this combination make the most senese?\n",
    "- Do you think, overall, ChatGPT-generated reviews are more posive or negative than human-generated ones? Use data to support your conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment(tokenized_docs, pos, neg ):\n",
    "  sentimentCol=[]\n",
    "  pos=list(pos[0])\n",
    "  neg=list(neg[0])\n",
    "  for doc in tokenized_docs:\n",
    "    positives = [token for token in doc if token in pos]\n",
    "    countPos=len(positives)\n",
    "    negatives = [token for token in doc if token in neg]\n",
    "    countNeg = len(negatives)\n",
    "    if countPos==0 and countNeg==0:\n",
    "      sentiment=0\n",
    "    else:\n",
    "      sentiment = (countPos-countNeg)/(countPos+countNeg)\n",
    "    sentimentCol.append(sentiment)\n",
    "  return sentimentCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0         a+\n",
       "1     abound\n",
       "2    abounds\n",
       "3  abundance\n",
       "4   abundant"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0     2-faced\n",
       "1     2-faces\n",
       "2    abnormal\n",
       "3     abolish\n",
       "4  abominable"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = pd.read_csv(\"positive-words-1.txt\", header = None)\n",
    "pos.head()\n",
    "\n",
    "neg = pd.read_csv(\"negative-words-1.txt\", header = None)\n",
    "neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.7272727272727273, 0.7777777777777778, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 1.0, 0.8666666666666667, 0.15789473684210525, 0.2, 1.0, 0.5789473684210527, 1.0, 1.0, 0.7142857142857143, 0.5454545454545454, 0.8666666666666667, 0.45454545454545453, 0.5555555555555556, 0.8461538461538461, 0.8461538461538461, 0.7142857142857143, 0.45454545454545453, 0.5, 0.3333333333333333, 0.8181818181818182, 0.8, 0.6842105263157895, 1.0, 0.0, 1.0, 0.75, 0.7333333333333333, 0.8571428571428571, 0.6666666666666666, 0.8461538461538461, 0.7142857142857143, 0.6666666666666666, 1.0, 0.6666666666666666, 0.55, 0.6, 0.5384615384615384, 0.7142857142857143, 1.0, 0.5384615384615384, 0.75, 0.18181818181818182, 0.6, 0.7142857142857143, 0.8181818181818182, 0.8461538461538461, 0.25, 1.0, -0.1111111111111111, 0.5, 0.25, 0.5555555555555556, 0.75, 1.0, 0.6923076923076923, 0.5714285714285714, 1.0, 1.0, 0.6, 0.7142857142857143, 1.0, 0.5, 0.3684210526315789, 1.0, 1.0, 1.0, 0.7142857142857143, 0.6, 0.2222222222222222, 0.3333333333333333, 0.5, 0.2, -0.2, 1.0, 0.8666666666666667, 0.3333333333333333, 1.0, 0.45454545454545453, 0.6923076923076923, 0.7333333333333333, 0.42857142857142855, 0.3333333333333333, 0.8, 0.2, 0.6, 0.25, 1.0, 1.0, 0.8461538461538461, 0.8666666666666667, 0.46153846153846156, 0.6, 0.6551724137931034, 1.0, 0.7368421052631579, 0.7777777777777778, 0.3333333333333333, 1.0, 1.0, 0.8823529411764706, 0.5714285714285714, 0.875, 0.9047619047619048, 0.25, 0.76, 1.0, 0.42857142857142855, 0.38461538461538464, 0.75, 0.2727272727272727, 0.8181818181818182, 0.6521739130434783, 0.2857142857142857, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6521739130434783, 0.8666666666666667, 0.8947368421052632, 0.2, 0.6363636363636364, 0.6842105263157895, 0.5833333333333334, 1.0, 0.75, 0.4444444444444444, 1.0, 0.6666666666666666, 0.6, 1.0, 0.38461538461538464, 1.0, 1.0, 0.45454545454545453, 0.8461538461538461, 0.8571428571428571, 0.047619047619047616, 0.4117647058823529, 0.75, 0.6, 1.0, 1.0, 0.5294117647058824, 0.8461538461538461, 1.0, 0.75, 0.6363636363636364, 0.6923076923076923, 1.0, 0.5, 0.8, 0.5384615384615384, 0.7777777777777778, 0.875, 0.5555555555555556, 0.75, 0.42857142857142855, 0.12, 0.3333333333333333, 1.0, 0.7142857142857143, 1.0, 1.0, 0.9047619047619048, 0.047619047619047616, 0.1111111111111111, 0.3333333333333333, 0.2, 0.0, 0.4, 0.42857142857142855, 0.3333333333333333, 1.0, -0.09090909090909091, -0.42857142857142855, 0.6842105263157895, 0.625, 1.0, -0.3333333333333333, 1.0, 0.8, 0.8461538461538461, 0.8, 0.0, 1.0, 0.7333333333333333, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.5, 0.6666666666666666, 0.3333333333333333, 1.0, 0.7142857142857143, 0.4, 0.6363636363636364, 0.625, 1.0, 0.3333333333333333, 1.0, 0.8947368421052632, 0.09090909090909091, 1.0, 0.38461538461538464, 0.3793103448275862, 0.7777777777777778, 0.75, 1.0, 0.2222222222222222, 0.8571428571428571, 1.0, 0.8666666666666667, 0.25, 1.0, 0.7142857142857143, 0.5714285714285714, 0.8571428571428571, 0.42857142857142855, 1.0, 0.7142857142857143, 0.875, 0.6, 1.0, 1.0, 0.2, 1.0, 0.6923076923076923, 0.5555555555555556, 0.5, 0.4, 1.0, 0.5, 0.2, 0.5, 0.6666666666666666, 0.8181818181818182, 0.5, 0.8333333333333334, 1.0, 0.7142857142857143, 1.0, 0.5, 0.75, 0.7333333333333333, 1.0, 0.5714285714285714, 0.23076923076923078, 0.8666666666666667, 1.0, 1.0, 0.2, 0.8666666666666667, 0.6, 0.3333333333333333, 0.8, 0.0, 0.3, 0.6363636363636364, 0.75, 0.7272727272727273, 0.3333333333333333, 0.8, 0.0, 0.7142857142857143, -0.17647058823529413, 1.0, 0.5862068965517241, 0.9047619047619048, 0.7894736842105263, 0.8571428571428571, 1.0, 0.0, 0.6842105263157895, 0.5555555555555556, 0.9047619047619048, 0.6, 0.14285714285714285, 0.11764705882352941, 0.7777777777777778, 1.0, 0.3684210526315789, 0.875, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# get tokens, try different parameters, for example:\n",
    "docs=data[\"text\"]\n",
    "# use your function\n",
    "tokenized_docs = docs.apply(tokenize)\n",
    "sentimentsList=compute_sentiment(tokenized_docs, pos, neg)\n",
    "print(sentimentsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.6842105263157895, 0.7777777777777778, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 1.0, 0.8571428571428571, 0.15789473684210525, 0.2, 1.0, 0.5789473684210527, 1.0, 1.0, 0.6666666666666666, 0.5454545454545454, 0.8666666666666667, 0.45454545454545453, 0.5555555555555556, 0.8461538461538461, 0.8333333333333334, 0.6666666666666666, 0.3333333333333333, 0.5, 0.3333333333333333, 0.8181818181818182, 0.8, 0.6842105263157895, 1.0, 0.0, 1.0, 0.75, 0.6923076923076923, 0.8571428571428571, 0.6521739130434783, 0.8333333333333334, 0.7142857142857143, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5263157894736842, 0.6, 0.5384615384615384, 0.7142857142857143, 1.0, 0.5384615384615384, 0.75, 0.18181818181818182, 0.6, 0.6470588235294118, 0.8181818181818182, 0.8461538461538461, 0.25, 1.0, -0.1111111111111111, 0.5, 0.25, 0.5555555555555556, 0.75, 1.0, 0.6923076923076923, 0.5714285714285714, 1.0, 1.0, 0.6, 0.7142857142857143, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6923076923076923, 0.5555555555555556, 0.2222222222222222, 0.3333333333333333, 0.5, 0.2, -0.2, 1.0, 0.8666666666666667, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.7333333333333333, 0.38461538461538464, 0.29411764705882354, 0.7777777777777778, 0.2, 0.5789473684210527, 0.14285714285714285, 1.0, 1.0, 0.8461538461538461, 0.8666666666666667, 0.44, 0.6, 0.6428571428571429, 1.0, 0.7297297297297297, 0.7777777777777778, 0.3333333333333333, 1.0, 1.0, 0.8823529411764706, 0.5714285714285714, 0.8666666666666667, 0.9047619047619048, 0.25, 0.76, 1.0, 0.42857142857142855, 0.3333333333333333, 0.7333333333333333, 0.2727272727272727, 0.8, 0.6190476190476191, 0.2857142857142857, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6363636363636364, 0.8571428571428571, 0.8888888888888888, 0.14285714285714285, 0.6363636363636364, 0.6842105263157895, 0.5833333333333334, 1.0, 0.75, 0.375, 1.0, 0.6666666666666666, 0.6, 1.0, 0.38461538461538464, 1.0, 1.0, 0.4, 0.8461538461538461, 0.8571428571428571, 0.047619047619047616, 0.3939393939393939, 0.7333333333333333, 0.6, 1.0, 1.0, 0.5, 0.8461538461538461, 1.0, 0.75, 0.6363636363636364, 0.6363636363636364, 1.0, 0.5, 0.8, 0.5384615384615384, 0.75, 0.875, 0.5555555555555556, 0.75, 0.3333333333333333, 0.12, 0.3333333333333333, 1.0, 0.7142857142857143, 1.0, 1.0, 0.9047619047619048, 0.047619047619047616, 0.1111111111111111, 0.2727272727272727, 0.2, -0.3333333333333333, 0.4, 0.42857142857142855, 0.3333333333333333, 1.0, -0.09090909090909091, -0.42857142857142855, 0.6842105263157895, 0.625, 1.0, -0.3333333333333333, 1.0, 0.8, 0.8461538461538461, 0.8, 0.0, 1.0, 0.6923076923076923, 0.2, 1.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.5, 0.6363636363636364, 0.3333333333333333, 1.0, 0.7142857142857143, 0.4, 0.6363636363636364, 0.625, 1.0, 0.3333333333333333, 1.0, 0.8823529411764706, 0.09090909090909091, 1.0, 0.35135135135135137, 0.3333333333333333, 0.7777777777777778, 0.75, 1.0, 0.17647058823529413, 0.8571428571428571, 1.0, 0.8571428571428571, 0.25, 1.0, 0.7142857142857143, 0.5714285714285714, 0.8571428571428571, 0.38461538461538464, 1.0, 0.7142857142857143, 0.8666666666666667, 0.5, 1.0, 1.0, 0.2, 1.0, 0.6923076923076923, 0.5555555555555556, 0.5, 0.4, 1.0, 0.5, 0.2, 0.3333333333333333, 0.6666666666666666, 0.8181818181818182, 0.5, 0.8333333333333334, 1.0, 0.7142857142857143, 1.0, 0.5, 0.75, 0.7142857142857143, 1.0, 0.5384615384615384, 0.09090909090909091, 0.8666666666666667, 1.0, 1.0, 0.2, 0.8666666666666667, 0.6, 0.3333333333333333, 0.7777777777777778, 0.0, 0.3, 0.6363636363636364, 0.75, 0.7142857142857143, 0.3333333333333333, 0.8, 0.0, 0.6666666666666666, -0.17647058823529413, 1.0, 0.5384615384615384, 0.9047619047619048, 0.7777777777777778, 0.8571428571428571, 1.0, -0.14285714285714285, 0.6666666666666666, 0.5555555555555556, 0.8888888888888888, 0.6, 0.0, 0.09090909090909091, 0.7777777777777778, 1.0, 0.3684210526315789, 0.875, 0.3333333333333333, 1.0, 0.8461538461538461, 1.0]\n"
     ]
    }
   ],
   "source": [
    "tokensNoStop = docs.apply(lambda x: tokenize(x, lemmatized=False, remove_stopword=True, remove_punct = False))\n",
    "sentimentNoStop = compute_sentiment(tokensNoStop, pos, neg)\n",
    "print(sentimentNoStop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.7272727272727273, 0.7777777777777778, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 1.0, 0.8666666666666667, 0.15789473684210525, 0.2, 1.0, 0.5789473684210527, 1.0, 1.0, 0.7142857142857143, 0.5454545454545454, 0.8666666666666667, 0.45454545454545453, 0.5555555555555556, 0.8461538461538461, 0.8461538461538461, 0.7142857142857143, 0.45454545454545453, 0.5, 0.3333333333333333, 0.8181818181818182, 0.8, 0.6842105263157895, 1.0, 0.0, 1.0, 0.75, 0.7333333333333333, 0.8571428571428571, 0.6666666666666666, 0.8461538461538461, 0.7142857142857143, 0.6666666666666666, 1.0, 0.6666666666666666, 0.55, 0.6, 0.5384615384615384, 0.7142857142857143, 1.0, 0.5384615384615384, 0.75, 0.18181818181818182, 0.6, 0.7142857142857143, 0.8181818181818182, 0.8461538461538461, 0.25, 1.0, -0.1111111111111111, 0.5, 0.25, 0.5555555555555556, 0.75, 1.0, 0.6923076923076923, 0.5714285714285714, 1.0, 1.0, 0.6, 0.7142857142857143, 1.0, 0.5, 0.3684210526315789, 1.0, 1.0, 1.0, 0.7142857142857143, 0.6, 0.2222222222222222, 0.3333333333333333, 0.5, 0.2, -0.2, 1.0, 0.8666666666666667, 0.3333333333333333, 1.0, 0.45454545454545453, 0.6923076923076923, 0.7333333333333333, 0.42857142857142855, 0.3333333333333333, 0.8, 0.2, 0.6, 0.25, 1.0, 1.0, 0.8461538461538461, 0.8666666666666667, 0.46153846153846156, 0.6, 0.6551724137931034, 1.0, 0.7368421052631579, 0.7777777777777778, 0.3333333333333333, 1.0, 1.0, 0.8823529411764706, 0.5714285714285714, 0.875, 0.9047619047619048, 0.25, 0.76, 1.0, 0.42857142857142855, 0.38461538461538464, 0.75, 0.2727272727272727, 0.8181818181818182, 0.6521739130434783, 0.2857142857142857, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6521739130434783, 0.8666666666666667, 0.8947368421052632, 0.2, 0.6363636363636364, 0.6842105263157895, 0.5833333333333334, 1.0, 0.75, 0.4444444444444444, 1.0, 0.6666666666666666, 0.6, 1.0, 0.38461538461538464, 1.0, 1.0, 0.45454545454545453, 0.8461538461538461, 0.8571428571428571, 0.047619047619047616, 0.4117647058823529, 0.75, 0.6, 1.0, 1.0, 0.5294117647058824, 0.8461538461538461, 1.0, 0.75, 0.6363636363636364, 0.6923076923076923, 1.0, 0.5, 0.8, 0.5384615384615384, 0.7777777777777778, 0.875, 0.5555555555555556, 0.75, 0.42857142857142855, 0.12, 0.3333333333333333, 1.0, 0.7142857142857143, 1.0, 1.0, 0.9047619047619048, 0.047619047619047616, 0.1111111111111111, 0.3333333333333333, 0.2, 0.0, 0.4, 0.42857142857142855, 0.3333333333333333, 1.0, -0.09090909090909091, -0.42857142857142855, 0.6842105263157895, 0.625, 1.0, -0.3333333333333333, 1.0, 0.8, 0.8461538461538461, 0.8, 0.0, 1.0, 0.7333333333333333, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.5, 0.6666666666666666, 0.3333333333333333, 1.0, 0.7142857142857143, 0.4, 0.6363636363636364, 0.625, 1.0, 0.3333333333333333, 1.0, 0.8947368421052632, 0.09090909090909091, 1.0, 0.38461538461538464, 0.3793103448275862, 0.7777777777777778, 0.75, 1.0, 0.2222222222222222, 0.8571428571428571, 1.0, 0.8666666666666667, 0.25, 1.0, 0.7142857142857143, 0.5714285714285714, 0.8571428571428571, 0.42857142857142855, 1.0, 0.7142857142857143, 0.875, 0.6, 1.0, 1.0, 0.2, 1.0, 0.6923076923076923, 0.5555555555555556, 0.5, 0.4, 1.0, 0.5, 0.2, 0.5, 0.6666666666666666, 0.8181818181818182, 0.5, 0.8333333333333334, 1.0, 0.7142857142857143, 1.0, 0.5, 0.75, 0.7333333333333333, 1.0, 0.5714285714285714, 0.23076923076923078, 0.8666666666666667, 1.0, 1.0, 0.2, 0.8666666666666667, 0.6, 0.3333333333333333, 0.8, 0.0, 0.3, 0.6363636363636364, 0.75, 0.7272727272727273, 0.3333333333333333, 0.8, 0.0, 0.7142857142857143, -0.17647058823529413, 1.0, 0.5862068965517241, 0.9047619047619048, 0.7894736842105263, 0.8571428571428571, 1.0, 0.0, 0.6842105263157895, 0.5555555555555556, 0.9047619047619048, 0.6, 0.14285714285714285, 0.11764705882352941, 0.7777777777777778, 1.0, 0.3684210526315789, 0.875, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0]\n"
     ]
    }
   ],
   "source": [
    "tokensNoPunct =  docs.apply(lambda x: tokenize(x, lemmatized=False, remove_stopword=False, remove_punct=True))\n",
    "sentimentNoPunct = compute_sentiment(tokensNoPunct, pos, neg)\n",
    "print(sentimentNoPunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.6842105263157895, 0.7777777777777778, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 1.0, 0.8571428571428571, 0.15789473684210525, 0.2, 1.0, 0.5789473684210527, 1.0, 1.0, 0.6666666666666666, 0.5454545454545454, 0.8666666666666667, 0.45454545454545453, 0.5555555555555556, 0.8461538461538461, 0.8333333333333334, 0.6666666666666666, 0.3333333333333333, 0.5, 0.3333333333333333, 0.8181818181818182, 0.8, 0.6842105263157895, 1.0, 0.0, 1.0, 0.75, 0.6923076923076923, 0.8571428571428571, 0.6521739130434783, 0.8333333333333334, 0.7142857142857143, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5263157894736842, 0.6, 0.5384615384615384, 0.7142857142857143, 1.0, 0.5384615384615384, 0.75, 0.18181818181818182, 0.6, 0.6470588235294118, 0.8181818181818182, 0.8461538461538461, 0.25, 1.0, -0.1111111111111111, 0.5, 0.25, 0.5555555555555556, 0.75, 1.0, 0.6923076923076923, 0.5714285714285714, 1.0, 1.0, 0.6, 0.7142857142857143, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6923076923076923, 0.5555555555555556, 0.2222222222222222, 0.3333333333333333, 0.5, 0.2, -0.2, 1.0, 0.8666666666666667, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.7333333333333333, 0.38461538461538464, 0.29411764705882354, 0.7777777777777778, 0.2, 0.5789473684210527, 0.14285714285714285, 1.0, 1.0, 0.8461538461538461, 0.8666666666666667, 0.44, 0.6, 0.6428571428571429, 1.0, 0.7297297297297297, 0.7777777777777778, 0.3333333333333333, 1.0, 1.0, 0.8823529411764706, 0.5714285714285714, 0.8666666666666667, 0.9047619047619048, 0.25, 0.76, 1.0, 0.42857142857142855, 0.3333333333333333, 0.7333333333333333, 0.2727272727272727, 0.8, 0.6190476190476191, 0.2857142857142857, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6363636363636364, 0.8571428571428571, 0.8888888888888888, 0.14285714285714285, 0.6363636363636364, 0.6842105263157895, 0.5833333333333334, 1.0, 0.75, 0.375, 1.0, 0.6666666666666666, 0.6, 1.0, 0.38461538461538464, 1.0, 1.0, 0.4, 0.8461538461538461, 0.8571428571428571, 0.047619047619047616, 0.3939393939393939, 0.7333333333333333, 0.6, 1.0, 1.0, 0.5, 0.8461538461538461, 1.0, 0.75, 0.6363636363636364, 0.6363636363636364, 1.0, 0.5, 0.8, 0.5384615384615384, 0.75, 0.875, 0.5555555555555556, 0.75, 0.3333333333333333, 0.12, 0.3333333333333333, 1.0, 0.7142857142857143, 1.0, 1.0, 0.9047619047619048, 0.047619047619047616, 0.1111111111111111, 0.2727272727272727, 0.2, -0.3333333333333333, 0.4, 0.42857142857142855, 0.3333333333333333, 1.0, -0.09090909090909091, -0.42857142857142855, 0.6842105263157895, 0.625, 1.0, -0.3333333333333333, 1.0, 0.8, 0.8461538461538461, 0.8, 0.0, 1.0, 0.6923076923076923, 0.2, 1.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.5, 0.6363636363636364, 0.3333333333333333, 1.0, 0.7142857142857143, 0.4, 0.6363636363636364, 0.625, 1.0, 0.3333333333333333, 1.0, 0.8823529411764706, 0.09090909090909091, 1.0, 0.35135135135135137, 0.3333333333333333, 0.7777777777777778, 0.75, 1.0, 0.17647058823529413, 0.8571428571428571, 1.0, 0.8571428571428571, 0.25, 1.0, 0.7142857142857143, 0.5714285714285714, 0.8571428571428571, 0.38461538461538464, 1.0, 0.7142857142857143, 0.8666666666666667, 0.5, 1.0, 1.0, 0.2, 1.0, 0.6923076923076923, 0.5555555555555556, 0.5, 0.4, 1.0, 0.5, 0.2, 0.3333333333333333, 0.6666666666666666, 0.8181818181818182, 0.5, 0.8333333333333334, 1.0, 0.7142857142857143, 1.0, 0.5, 0.75, 0.7142857142857143, 1.0, 0.5384615384615384, 0.09090909090909091, 0.8666666666666667, 1.0, 1.0, 0.2, 0.8666666666666667, 0.6, 0.3333333333333333, 0.7777777777777778, 0.0, 0.3, 0.6363636363636364, 0.75, 0.7142857142857143, 0.3333333333333333, 0.8, 0.0, 0.6666666666666666, -0.17647058823529413, 1.0, 0.5384615384615384, 0.9047619047619048, 0.7777777777777778, 0.8571428571428571, 1.0, -0.14285714285714285, 0.6666666666666666, 0.5555555555555556, 0.8888888888888888, 0.6, 0.0, 0.09090909090909091, 0.7777777777777778, 1.0, 0.3684210526315789, 0.875, 0.3333333333333333, 1.0, 0.8461538461538461, 1.0]\n"
     ]
    }
   ],
   "source": [
    "tokensNoPnoS=  docs.apply(lambda x: tokenize(x, lemmatized=False, remove_stopword=True, remove_punct=True))\n",
    "sentimentNoPnoS = compute_sentiment(tokensNoPnoS, pos, neg)\n",
    "print(sentimentNoPnoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 0.7142857142857143, 0.6, 0.6666666666666666, 0.8, 0.6666666666666666, 1.0, 0.5714285714285714, 0.7142857142857143, 1.0, 0.8666666666666667, 0.2222222222222222, 0.5, 1.0, 0.5, 1.0, 1.0, 0.7142857142857143, 0.5454545454545454, 0.75, 0.38461538461538464, 0.6, 0.8666666666666667, 0.8461538461538461, 0.5, 0.45454545454545453, 0.5, 0.3333333333333333, 0.8181818181818182, 0.6363636363636364, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5555555555555556, 0.7333333333333333, 0.7333333333333333, 0.68, 0.8461538461538461, 0.6, 0.6, 1.0, 0.6666666666666666, 0.5238095238095238, 0.6, 0.5714285714285714, 0.8461538461538461, 1.0, 0.4666666666666667, 1.0, 0.18181818181818182, 0.6, 0.7272727272727273, 0.8181818181818182, 0.8571428571428571, 0.25, 1.0, -0.05263157894736842, 0.5454545454545454, 0.42857142857142855, 0.5555555555555556, 0.75, 0.8, 0.6923076923076923, 0.4117647058823529, 1.0, 1.0, 0.6, 0.7142857142857143, 0.7777777777777778, 0.6363636363636364, 0.4, 1.0, 0.5, 1.0, 0.7142857142857143, 0.6, 0.3333333333333333, 0.3333333333333333, 0.5, 0.5, -0.2, 1.0, 0.8666666666666667, 0.3333333333333333, 1.0, 0.45454545454545453, 0.6666666666666666, 0.7333333333333333, 0.25, 0.3333333333333333, 0.8, 0.09090909090909091, 0.6, 0.25, 1.0, 1.0, 0.7142857142857143, 0.8571428571428571, 0.48148148148148145, 0.6, 0.6774193548387096, 1.0, 0.7619047619047619, 0.7777777777777778, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6, 0.875, 0.6923076923076923, 0.25, 0.75, 1.0, 0.45454545454545453, 0.5, 0.75, 0.2727272727272727, 0.8, 0.68, 0.2857142857142857, 0.6, 0.8461538461538461, 1.0, 1.0, 1.0, 0.5833333333333334, 1.0, 0.9047619047619048, 0.2, 0.6363636363636364, 0.6842105263157895, 0.5833333333333334, 1.0, 0.75, 0.5294117647058824, 1.0, 0.6666666666666666, 0.6, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6, 0.8571428571428571, 0.8571428571428571, 0.1, 0.45454545454545453, 0.75, 0.6, 1.0, 1.0, 0.5294117647058824, 0.8461538461538461, 1.0, 0.75, 0.6666666666666666, 0.7333333333333333, 0.8181818181818182, 0.6363636363636364, 0.8, 0.5714285714285714, 1.0, 0.8823529411764706, 0.5, 0.75, 0.42857142857142855, 0.07692307692307693, 0.36363636363636365, 1.0, 0.8461538461538461, 0.7777777777777778, 0.75, 0.9047619047619048, 0.09090909090909091, 0.2, 0.3333333333333333, 0.2, -0.2, 0.45454545454545453, 0.42857142857142855, 0.3333333333333333, 1.0, -0.09090909090909091, -0.5, 0.6842105263157895, 0.625, 1.0, -0.3333333333333333, 1.0, 0.8, 0.8571428571428571, 0.8, 1.0, 1.0, 0.8571428571428571, 0.3333333333333333, 0.8666666666666667, 1.0, 1.0, 0.3684210526315789, 1.0, 0.5, 0.6666666666666666, 0.6, 1.0, 0.7142857142857143, 0.4, 0.8, 0.625, 1.0, 0.3333333333333333, 1.0, 0.9, 0.09090909090909091, 1.0, 0.45, 0.4482758620689655, 0.7777777777777778, 0.6363636363636364, 1.0, 0.2222222222222222, 0.7333333333333333, 1.0, 0.8666666666666667, 0.3333333333333333, 0.8, 0.7142857142857143, 0.6, 0.875, 0.4666666666666667, 0.9230769230769231, 0.7142857142857143, 0.7777777777777778, 0.6, 0.75, 1.0, 0.2, 1.0, 0.6923076923076923, 0.5555555555555556, 0.5, 0.45454545454545453, 1.0, 0.25, 0.2, 0.5, 0.696969696969697, 0.8181818181818182, 0.7142857142857143, 0.8333333333333334, 1.0, 0.6470588235294118, 1.0, 0.38461538461538464, 0.75, 0.7333333333333333, 1.0, 0.5714285714285714, 0.14285714285714285, 1.0, 0.8888888888888888, 1.0, 0.2, 1.0, 0.6, 0.3333333333333333, 0.8, -0.06666666666666667, 0.3, 0.8, 0.75, 0.7391304347826086, 0.3333333333333333, 0.6363636363636364, 0.2727272727272727, 0.75, -0.17647058823529413, 1.0, 0.6666666666666666, 0.8181818181818182, 0.7894736842105263, 0.75, 1.0, 0.0, 0.5833333333333334, 1.0, 0.8260869565217391, 0.7777777777777778, 0.25, 0.11764705882352941, 0.45454545454545453, 1.0, 0.4444444444444444, 0.6842105263157895, 0.3333333333333333, 0.8, 0.8666666666666667, 1.0]\n"
     ]
    }
   ],
   "source": [
    "tokensLemmatized =  docs.apply(lambda x: tokenize(x, lemmatized=True, remove_stopword=False, remove_punct=False))\n",
    "sentimentLemmatized = compute_sentiment(tokensLemmatized, pos, neg)\n",
    "print(sentimentLemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 0.875, 0.5555555555555556, 0.6, 0.8, 0.6666666666666666, 1.0, 0.5714285714285714, 0.5, 1.0, 0.8571428571428571, 0.2, 0.5, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.6190476190476191, 0.75, 0.5454545454545454, 0.6, 0.8666666666666667, 1.0, 0.42857142857142855, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.8181818181818182, 0.6363636363636364, 0.7777777777777778, 1.0, 0.0, 1.0, 0.75, 0.6923076923076923, 0.7142857142857143, 0.6521739130434783, 0.8181818181818182, 0.6, 0.6, 1.0, 0.6666666666666666, 0.5384615384615384, 0.6, 0.5714285714285714, 0.8461538461538461, 1.0, 0.4666666666666667, 1.0, 0.14285714285714285, 0.3333333333333333, 0.6470588235294118, 0.8181818181818182, 0.8666666666666667, 0.42857142857142855, 1.0, -0.1111111111111111, 0.5238095238095238, 0.25, 0.4, 0.75, 0.7777777777777778, 0.6923076923076923, 0.375, 1.0, 1.0, 0.6, 0.7142857142857143, 0.75, 0.6363636363636364, 0.3333333333333333, 1.0, 0.3333333333333333, 0.7142857142857143, 0.5384615384615384, 0.4666666666666667, 0.3, 0.3333333333333333, 0.5, 0.5, -0.2, 1.0, 0.8666666666666667, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.7333333333333333, 0.2, 0.25, 0.75, 0.09090909090909091, 0.5555555555555556, 0.14285714285714285, 1.0, 1.0, 0.7142857142857143, 0.8666666666666667, 0.46153846153846156, 0.6, 0.6551724137931034, 1.0, 0.75, 0.75, 0.3333333333333333, 1.0, 1.0, 0.8823529411764706, 0.5714285714285714, 1.0, 0.6923076923076923, 0.25, 0.75, 1.0, 0.45454545454545453, 0.45454545454545453, 0.6666666666666666, 0.2727272727272727, 0.7777777777777778, 0.6666666666666666, 0.23076923076923078, 0.6, 0.8181818181818182, 1.0, 1.0, 1.0, 0.6, 1.0, 0.8947368421052632, 0.14285714285714285, 0.6, 0.625, 0.7272727272727273, 1.0, 0.75, 0.42857142857142855, 1.0, 0.6666666666666666, 0.6, 1.0, 0.2857142857142857, 1.0, 1.0, 0.75, 0.8571428571428571, 0.8571428571428571, 0.1, 0.3939393939393939, 0.7142857142857143, 0.6, 1.0, 1.0, 0.6, 0.8461538461538461, 1.0, 0.75, 0.6363636363636364, 0.6923076923076923, 0.8, 0.6363636363636364, 0.8095238095238095, 0.5714285714285714, 0.7142857142857143, 0.875, 0.6666666666666666, 0.75, 0.3333333333333333, 0.04, 0.3684210526315789, 1.0, 0.8461538461538461, 0.7777777777777778, 0.7142857142857143, 1.0, 0.09090909090909091, 0.25, 0.2727272727272727, 0.0, -0.5, 0.45454545454545453, 0.42857142857142855, 0.3333333333333333, 1.0, -0.09090909090909091, -0.5, 0.6666666666666666, 0.8571428571428571, 1.0, -0.3333333333333333, 1.0, 0.8, 0.8461538461538461, 0.8, 1.0, 1.0, 0.8333333333333334, 0.2, 0.8461538461538461, 1.0, 1.0, 0.4, 1.0, 0.6, 0.6363636363636364, 0.7777777777777778, 1.0, 0.7142857142857143, 0.4, 0.8, 0.6, 1.0, 0.2857142857142857, 1.0, 0.8888888888888888, 0.09090909090909091, 1.0, 0.40540540540540543, 0.52, 0.7777777777777778, 0.6363636363636364, 1.0, 0.17647058823529413, 0.6923076923076923, 1.0, 0.8666666666666667, 0.1111111111111111, 0.8181818181818182, 0.7142857142857143, 0.5714285714285714, 0.875, 0.42857142857142855, 0.92, 0.7142857142857143, 0.7647058823529411, 0.42857142857142855, 0.75, 1.0, 0.2, 1.0, 0.6923076923076923, 0.5, 0.5789473684210527, 0.5555555555555556, 1.0, 0.42857142857142855, 0.2, 0.3333333333333333, 0.696969696969697, 0.8, 0.5, 0.8333333333333334, 1.0, 0.6470588235294118, 1.0, 0.45454545454545453, 0.75, 0.6923076923076923, 1.0, 0.5384615384615384, 0.1111111111111111, 1.0, 0.8888888888888888, 1.0, 0.30434782608695654, 1.0, 0.6, 0.3333333333333333, 0.8, -0.07692307692307693, 0.3, 0.8, 0.75, 0.7142857142857143, 0.3333333333333333, 0.6363636363636364, 0.2727272727272727, 0.7142857142857143, -0.25, 1.0, 0.6923076923076923, 0.8181818181818182, 0.7777777777777778, 0.75, 1.0, -0.14285714285714285, 0.6190476190476191, 1.0, 0.8, 0.75, 0.14285714285714285, 0.125, 0.45454545454545453, 1.0, 0.375, 0.6666666666666666, 0.0, 0.6363636363636364, 0.8461538461538461, 1.0]\n"
     ]
    }
   ],
   "source": [
    "tokensAllTrue =  docs.apply(lambda x: tokenize(x, lemmatized=True, remove_stopword=True, remove_punct=True))\n",
    "sentimentAllTrue = compute_sentiment(tokensAllTrue, pos, neg)\n",
    "print(sentimentAllTrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'enough', 'top']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "common = [x for x in spacy_stopwords if x in list(pos[0]) or x in list(neg[0])]\n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>concrete</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentimentNoPunct</th>\n",
       "      <th>sentimentNoStop</th>\n",
       "      <th>sentimentNoPnoS</th>\n",
       "      <th>sentimentLemmatized</th>\n",
       "      <th>sentimentAllTrue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is delicious and extremely flavorful....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, so I didn't expect much in terms of Kore...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently visited the new Veggie Grill in NYC...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm a fan of 5Napkin Burger for 2 distinct tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe's Pizza offers the finest slice in Times S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I recently dined at this gem of a restaurant a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you like pad Thai, I highly recommend this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tucked away off the beaten path, this eatery o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.451327</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The hostess was gracious and the place was emp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Square pepperoni pizza?!? Sayyyy what?!? Stayi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  concrete  \\\n",
       "0  The food is delicious and extremely flavorful....      0  0.227941   \n",
       "1  Okay, so I didn't expect much in terms of Kore...      0  0.303030   \n",
       "2  I recently visited the new Veggie Grill in NYC...      1  0.307692   \n",
       "3  I'm a fan of 5Napkin Burger for 2 distinct tim...      0  0.284091   \n",
       "4  Joe's Pizza offers the finest slice in Times S...      1  0.326087   \n",
       "5  I recently dined at this gem of a restaurant a...      1  0.423077   \n",
       "6  If you like pad Thai, I highly recommend this ...      0  0.230769   \n",
       "7  Tucked away off the beaten path, this eatery o...      1  0.451327   \n",
       "8  The hostess was gracious and the place was emp...      0  0.219512   \n",
       "9  Square pepperoni pizza?!? Sayyyy what?!? Stayi...      0  0.290598   \n",
       "\n",
       "   sentiment  sentimentNoPunct  sentimentNoStop  sentimentNoPnoS  \\\n",
       "0   0.250000          0.250000         0.250000         0.250000   \n",
       "1   0.727273          0.727273         0.684211         0.684211   \n",
       "2   0.777778          0.777778         0.777778         0.777778   \n",
       "3   0.666667          0.666667         0.666667         0.666667   \n",
       "4   1.000000          1.000000         1.000000         1.000000   \n",
       "5   0.666667          0.666667         0.666667         0.666667   \n",
       "6   1.000000          1.000000         1.000000         1.000000   \n",
       "7   0.500000          0.500000         0.500000         0.500000   \n",
       "8   0.500000          0.500000         0.500000         0.500000   \n",
       "9   1.000000          1.000000         1.000000         1.000000   \n",
       "\n",
       "   sentimentLemmatized  sentimentAllTrue  \n",
       "0             0.400000          0.400000  \n",
       "1             0.714286          0.875000  \n",
       "2             0.600000          0.555556  \n",
       "3             0.666667          0.600000  \n",
       "4             0.800000          0.800000  \n",
       "5             0.666667          0.666667  \n",
       "6             1.000000          1.000000  \n",
       "7             0.571429          0.571429  \n",
       "8             0.714286          0.500000  \n",
       "9             1.000000          1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare between ChatGPT-generated reviews and human reviews\n",
    "data['sentiment'] = sentimentsList\n",
    "data['sentimentNoPunct'] = sentimentNoPunct\n",
    "data['sentimentNoStop'] = sentimentNoStop\n",
    "data['sentimentNoPnoS'] = sentimentNoPnoS\n",
    "data['sentimentLemmatized'] = sentimentLemmatized\n",
    "data['sentimentAllTrue'] = sentimentAllTrue\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>concrete</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentimentNoStop</th>\n",
       "      <th>sentimentLemmatized</th>\n",
       "      <th>sentimentAllTrue</th>\n",
       "      <th>mean_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is delicious and extremely flavorful....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, so I didn't expect much in terms of Kore...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.750192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently visited the new Veggie Grill in NYC...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.677778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm a fan of 5Napkin Burger for 2 distinct tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe's Pizza offers the finest slice in Times S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>I recently dined at this charming spot for a w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.775219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>IHop has better food the further you go down s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Favorite new spot. Everything was really good ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.859091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Tavolino, the sibling establishment to the bel...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.401639</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.854029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>I had this place on my to do list for the long...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.248485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  concrete  \\\n",
       "0    The food is delicious and extremely flavorful....      0  0.227941   \n",
       "1    Okay, so I didn't expect much in terms of Kore...      0  0.303030   \n",
       "2    I recently visited the new Veggie Grill in NYC...      1  0.307692   \n",
       "3    I'm a fan of 5Napkin Burger for 2 distinct tim...      0  0.284091   \n",
       "4    Joe's Pizza offers the finest slice in Times S...      1  0.326087   \n",
       "..                                                 ...    ...       ...   \n",
       "295  I recently dined at this charming spot for a w...      1  0.323529   \n",
       "296  IHop has better food the further you go down s...      0  0.194444   \n",
       "297  Favorite new spot. Everything was really good ...      0  0.262295   \n",
       "298  Tavolino, the sibling establishment to the bel...      1  0.401639   \n",
       "299  I had this place on my to do list for the long...      0  0.248485   \n",
       "\n",
       "     sentiment  sentimentNoStop  sentimentLemmatized  sentimentAllTrue  \\\n",
       "0     0.250000         0.250000             0.400000          0.400000   \n",
       "1     0.727273         0.684211             0.714286          0.875000   \n",
       "2     0.777778         0.777778             0.600000          0.555556   \n",
       "3     0.666667         0.666667             0.666667          0.600000   \n",
       "4     1.000000         1.000000             0.800000          0.800000   \n",
       "..         ...              ...                  ...               ...   \n",
       "295   0.875000         0.875000             0.684211          0.666667   \n",
       "296   0.333333         0.333333             0.333333          0.000000   \n",
       "297   1.000000         1.000000             0.800000          0.636364   \n",
       "298   0.857143         0.846154             0.866667          0.846154   \n",
       "299   1.000000         1.000000             1.000000          1.000000   \n",
       "\n",
       "     mean_sentiment  \n",
       "0          0.325000  \n",
       "1          0.750192  \n",
       "2          0.677778  \n",
       "3          0.650000  \n",
       "4          0.900000  \n",
       "..              ...  \n",
       "295        0.775219  \n",
       "296        0.250000  \n",
       "297        0.859091  \n",
       "298        0.854029  \n",
       "299        1.000000  \n",
       "\n",
       "[300 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on comparing the columns in the table above, we can see that removing punctuation has no bearning on sentiment score (as expected)  so we should keep columns that \n",
    "# do remove punctuation for computational purposes and focus mostly on the difference between lemmatization, removing stop words, or doing both\n",
    "\n",
    "data.drop(columns=[\"sentimentNoPunct\", \"sentimentNoPnoS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>concrete</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentimentNoPunct</th>\n",
       "      <th>sentimentNoStop</th>\n",
       "      <th>sentimentNoPnoS</th>\n",
       "      <th>sentimentLemmatized</th>\n",
       "      <th>sentimentAllTrue</th>\n",
       "      <th>mean_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food is delicious and extremely flavorful....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227941</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, so I didn't expect much in terms of Kore...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.750192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently visited the new Veggie Grill in NYC...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.677778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm a fan of 5Napkin Burger for 2 distinct tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe's Pizza offers the finest slice in Times S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I recently dined at this gem of a restaurant a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you like pad Thai, I highly recommend this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tucked away off the beaten path, this eatery o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.451327</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The hostess was gracious and the place was emp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.553571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Square pepperoni pizza?!? Sayyyy what?!? Stayi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  concrete  \\\n",
       "0  The food is delicious and extremely flavorful....      0  0.227941   \n",
       "1  Okay, so I didn't expect much in terms of Kore...      0  0.303030   \n",
       "2  I recently visited the new Veggie Grill in NYC...      1  0.307692   \n",
       "3  I'm a fan of 5Napkin Burger for 2 distinct tim...      0  0.284091   \n",
       "4  Joe's Pizza offers the finest slice in Times S...      1  0.326087   \n",
       "5  I recently dined at this gem of a restaurant a...      1  0.423077   \n",
       "6  If you like pad Thai, I highly recommend this ...      0  0.230769   \n",
       "7  Tucked away off the beaten path, this eatery o...      1  0.451327   \n",
       "8  The hostess was gracious and the place was emp...      0  0.219512   \n",
       "9  Square pepperoni pizza?!? Sayyyy what?!? Stayi...      0  0.290598   \n",
       "\n",
       "   sentiment  sentimentNoPunct  sentimentNoStop  sentimentNoPnoS  \\\n",
       "0   0.250000          0.250000         0.250000         0.250000   \n",
       "1   0.727273          0.727273         0.684211         0.684211   \n",
       "2   0.777778          0.777778         0.777778         0.777778   \n",
       "3   0.666667          0.666667         0.666667         0.666667   \n",
       "4   1.000000          1.000000         1.000000         1.000000   \n",
       "5   0.666667          0.666667         0.666667         0.666667   \n",
       "6   1.000000          1.000000         1.000000         1.000000   \n",
       "7   0.500000          0.500000         0.500000         0.500000   \n",
       "8   0.500000          0.500000         0.500000         0.500000   \n",
       "9   1.000000          1.000000         1.000000         1.000000   \n",
       "\n",
       "   sentimentLemmatized  sentimentAllTrue  mean_sentiment  \n",
       "0             0.400000          0.400000        0.325000  \n",
       "1             0.714286          0.875000        0.750192  \n",
       "2             0.600000          0.555556        0.677778  \n",
       "3             0.666667          0.600000        0.650000  \n",
       "4             0.800000          0.800000        0.900000  \n",
       "5             0.666667          0.666667        0.666667  \n",
       "6             1.000000          1.000000        1.000000  \n",
       "7             0.571429          0.571429        0.535714  \n",
       "8             0.714286          0.500000        0.553571  \n",
       "9             1.000000          1.000000        1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['mean_sentiment'] = data[['sentiment', 'sentimentNoStop','sentimentLemmatized', 'sentimentAllTrue']].mean(axis=1)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sentiment score for human reviews: 0.6445651959414616\n",
      "Mean sentiment score for GPT text: 0.6618849946640805\n",
      "mean human lemmatized:  0.6569835523952601\n",
      "mean gpt lemmatized:  0.6542593729552788\n",
      "mean human allTrue:  0.640534850156153\n",
      "mean gpt allTrue:  0.6430618691186898\n"
     ]
    }
   ],
   "source": [
    "mean_human = data[data['label'] == 0]['mean_sentiment'].mean()\n",
    "mean_gpt = data[data['label'] == 1]['mean_sentiment'].mean()\n",
    "print(\"Mean sentiment score for human reviews:\", mean_human)\n",
    "print(\"Mean sentiment score for GPT text:\", mean_gpt)\n",
    "\n",
    "\n",
    "\n",
    "mean_human_lemmatized = data[data['label'] == 0]['sentimentLemmatized'].mean()\n",
    "mean_gpt_lemmatized = data[data['label'] == 1]['sentimentLemmatized'].mean()\n",
    "print(\"mean human lemmatized: \", mean_human_lemmatized)\n",
    "print(\"mean gpt lemmatized: \", mean_gpt_lemmatized)\n",
    "\n",
    "\n",
    "mean_human_allTrue = data[data['label'] == 0]['sentimentAllTrue'].mean()\n",
    "mean_gpt_allTrue = data[data['label'] == 1]['sentimentAllTrue'].mean()\n",
    "print(\"mean human allTrue: \", mean_human_allTrue)\n",
    "print(\"mean gpt allTrue: \", mean_gpt_allTrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Compute TF-IDF and Compare Top Adjectives `(2 point)`\n",
    "\n",
    "Define a function `compute_tf_idf(tokenized_docs)` as follows: \n",
    "- Take paramter `tokenized_docs`, i.e., a list of tokenized documents by `tokenize` function in Q1\n",
    "- Calculate tf_idf weights as shown in lecture notes \n",
    "- Return the smoothed normalized `tf_idf` array, where each row stands for a document and each column denotes a word.\n",
    "- Use the `tokenize` function in Q1 with pos tags, extract the top adjectives by tfidf weights for GPT reviews, and do the same for human reviews. Can you see any differences? What types of adjectives does ChatGPT prefer? Explain in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(tokenized_docs):\n",
    "    #calculate tf_idf\n",
    "# Step 1. get tokens of each document as list   \n",
    "# create token count dictionary\n",
    "# step 2. process all documents to get a dictionary of dictionaries\n",
    "    docs_tokens={idx: nltk.FreqDist(doc) for idx,doc in enumerate(tokenized_docs)}\n",
    "    # print(docs_tokens)\n",
    "# step 3. get document-term matrix, contruct a document-term matrix where each row is a doc each column is a token and the value is the frequency of the token\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\")\n",
    "    dtm=dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0) # sort by index (i.e. doc id)\n",
    "# step 4. get normalized term frequency (tf) matrix, convert dtm to numpy arrays\n",
    "    dtm2=dtm.values\n",
    "    doc_len=dtm2.sum(axis=1) # sum the value of each row\n",
    "# divide dtm matrix by the doc length matrix\n",
    "    tf=np.divide(dtm2, doc_len[:,None])  # set float precision to print nicely\n",
    "    np.set_printoptions(precision=2)\n",
    "# step 5. get idf get document frequency\n",
    "    df=np.where(dtm2>0,1,0)\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    " # step 6. get tf-idf, by default normalize by row\n",
    "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "    return smoothed_tf_idf    # return smoothed normalized tf_idf array where each row stands for a document and each column denotes a word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.lemmatized=False, remove_stopword=False, remove_punct = True:\n",
      " Shape: (300, 48831)\n",
      "\n",
      "2.lemmatized=True, remove_stopword=True, remove_punct = True:\n",
      " Shape: (300, 4020)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different tokenization options to see how these options affect TFIDF matrix:\n",
    "tokens1 = data[\"text\"].apply(lambda x: tokenize(x, lemmatized=False, remove_stopword=False, remove_punct = False, pos_tag = False))\n",
    "dtm1 = compute_tfidf(tokens1)\n",
    "print(f\"1.lemmatized=False, remove_stopword=False, remove_punct = True:\\n \\\n",
    "Shape: {dtm1.shape}\\n\")\n",
    "\n",
    "\n",
    "tokens2 = data[\"text\"].apply(lambda x: tokenize(x, lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = False))\n",
    "dtm2 = compute_tfidf(tokens2)\n",
    "print(f\"2.lemmatized=True, remove_stopword=True, remove_punct = True:\\n \\\n",
    "Shape: {dtm2.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tokenize function in Q1 with pos tags\n",
    "tokens3 = data[\"text\"].apply(lambda x: tokenize(x, lemmatized=True, remove_stopword=True, remove_punct = True, pos_tag = True))\n",
    "dtm3 = compute_tfidf(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 words for ChatGPP reviews: [('delightful', 0.036282600079039454), ('perfect', 0.020108343933400736), ('culinary', 0.019677386314729), ('exceptional', 0.01847081801336628), ('flavorful', 0.017229612475560308), ('short', 0.01580233980043591), ('satisfying', 0.015228795036514574), ('charming', 0.013435044398147465), ('pleasant', 0.013299705117961734), ('cozy', 0.01285143061425164), ('worth', 0.012482041226243723), ('enjoyable', 0.012410384009811218), ('classic', 0.011914560557052473), ('attentive', 0.010968466561321439), ('local', 0.010921851806039296), ('recent', 0.010803043588672578), ('quick', 0.010671676294506128), ('impressive', 0.010629782650871002), ('decent', 0.010627216418500953), ('japanese', 0.010227080091596617)]\n",
      "\n",
      "top 20 words for Human reviews: [('good', 0.04456433434603564), ('great', 0.029143511024871446), ('delicious', 0.02279444210006634), ('nice', 0.02145287434723795), ('amazing', 0.01749131399980889), ('small', 0.015366299020570089), ('little', 0.014917704282437392), ('fresh', 0.014074743453454754), ('tasty', 0.013709992381909094), ('sweet', 0.013317979453075498), ('happy', 0.012374864438691787), ('spicy', 0.011660462485702015), ('excellent', 0.011234196916011723), ('worth', 0.010802358032635), ('hot', 0.010584396127421984), ('perfect', 0.010178087556784796), ('friendly', 0.009697105857481698), ('large', 0.009662485510630831), ('big', 0.00923265888291004), ('bad', 0.008874807272838039)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the top adjectives by tfidf weights for GPT reviews, and do the same for human reviews. \n",
    "# Can you see any differences? Explain in text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (Bonus): Further Analysis (Open question, 2 points)\n",
    "\n",
    "\n",
    "Can you investigate the following linguistic differences between human and chatgpt-generated answers:\n",
    "- Readability\n",
    "- Coherence\n",
    "\n",
    "You need to implement your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
