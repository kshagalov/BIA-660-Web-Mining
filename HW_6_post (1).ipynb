{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Sentiment Analysis & Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you'll need dataset:\n",
    "- `hw6_train.csv`: dataset for training\n",
    "- `hw6_test.csv`: dataset for test\n",
    "\n",
    "A snippet of the dataset is given below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Getting ready for college. I had a good sleep....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>We are having a party now to have all the fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@marC0110 ummm.. i see you.. and i really wann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@saboteur1 Thanks for following  Much apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Why eat at home? Picnic plans for today are al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0</td>\n",
       "      <td>@kaylacelina why so sad?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>1</td>\n",
       "      <td>@annahawkins Plus, we have Panera and Powells.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0</td>\n",
       "      <td>About to ge ready for work as i have a 9 hour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0</td>\n",
       "      <td>i think I may have broken my finger!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>1</td>\n",
       "      <td>@ehasselbeck Trying to compile a list of celeb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0          1  Getting ready for college. I had a good sleep....\n",
       "1          1  We are having a party now to have all the fami...\n",
       "2          0  @marC0110 ummm.. i see you.. and i really wann...\n",
       "3          1  @saboteur1 Thanks for following  Much apprecia...\n",
       "4          1  Why eat at home? Picnic plans for today are al...\n",
       "...      ...                                                ...\n",
       "59995      0                          @kaylacelina why so sad? \n",
       "59996      1    @annahawkins Plus, we have Panera and Powells. \n",
       "59997      0  About to ge ready for work as i have a 9 hour ...\n",
       "59998      0              i think I may have broken my finger! \n",
       "59999      1  @ehasselbeck Trying to compile a list of celeb...\n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"hw6_train.csv\")\n",
    "test = pd.read_csv(\"hw6_test.csv\")\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Unsupervised Sentiment Analysis (4 points)\n",
    "\n",
    "- Write a function `analyze_sentiment(docs, labels, th)` as follows: (3 points)\n",
    "    - Takes three inputs:\n",
    "       - `docs` : a list of documents, \n",
    "       - `labels` the ground-truth sentiment labels of `docs`\n",
    "       - `th`: compound threshold\n",
    "    - Use Vader to get a compound score of for each document in `docs`.  \n",
    "    - If `compound score > th`, then the predicted label is 1; otherwise 0\n",
    "    - Print out the classification report\n",
    "    - Return F1 macro score\n",
    "\n",
    "\n",
    "- Tune `th` such that the F1 macro score is maximimized (1 point)\n",
    "- With the `th` tuned, calculate the performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import logging\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(docs, labels, th=0):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    compound_scores = [sia.polarity_scores(doc)['compound'] for doc in docs]\n",
    "    predicted_labels = [1 if score > th else 0 for score in compound_scores]\n",
    "    print(classification_report(labels, predicted_labels))\n",
    "    f1 = f1_score(labels, predicted_labels, average='macro')\n",
    "                \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kshag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kshag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     29888\n",
      "           1       0.50      1.00      0.67     30112\n",
      "\n",
      "    accuracy                           0.50     60000\n",
      "   macro avg       0.25      0.50      0.33     60000\n",
      "weighted avg       0.25      0.50      0.34     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.00      0.01     29888\n",
      "           1       0.50      1.00      0.67     30112\n",
      "\n",
      "    accuracy                           0.50     60000\n",
      "   macro avg       0.71      0.50      0.34     60000\n",
      "weighted avg       0.71      0.50      0.34     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.03      0.05     29888\n",
      "           1       0.51      1.00      0.67     30112\n",
      "\n",
      "    accuracy                           0.51     60000\n",
      "   macro avg       0.70      0.51      0.36     60000\n",
      "weighted avg       0.70      0.51      0.36     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.06      0.12     29888\n",
      "           1       0.52      0.99      0.68     30112\n",
      "\n",
      "    accuracy                           0.53     60000\n",
      "   macro avg       0.70      0.53      0.40     60000\n",
      "weighted avg       0.69      0.53      0.40     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.11      0.20     29888\n",
      "           1       0.53      0.98      0.69     30112\n",
      "\n",
      "    accuracy                           0.55     60000\n",
      "   macro avg       0.70      0.55      0.44     60000\n",
      "weighted avg       0.70      0.55      0.44     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.18      0.30     29888\n",
      "           1       0.54      0.97      0.70     30112\n",
      "\n",
      "    accuracy                           0.58     60000\n",
      "   macro avg       0.70      0.57      0.50     60000\n",
      "weighted avg       0.70      0.58      0.50     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.26      0.39     29888\n",
      "           1       0.56      0.95      0.71     30112\n",
      "\n",
      "    accuracy                           0.61     60000\n",
      "   macro avg       0.70      0.60      0.55     60000\n",
      "weighted avg       0.70      0.61      0.55     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.31      0.45     29888\n",
      "           1       0.58      0.94      0.71     30112\n",
      "\n",
      "    accuracy                           0.62     60000\n",
      "   macro avg       0.70      0.62      0.58     60000\n",
      "weighted avg       0.70      0.62      0.58     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.37      0.51     29888\n",
      "           1       0.59      0.92      0.72     30112\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.71      0.64      0.61     60000\n",
      "weighted avg       0.70      0.64      0.61     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.40      0.54     29888\n",
      "           1       0.61      0.91      0.73     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.71      0.66      0.63     60000\n",
      "weighted avg       0.71      0.66      0.63     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.43      0.56     29888\n",
      "           1       0.61      0.90      0.73     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.71      0.66      0.64     60000\n",
      "weighted avg       0.71      0.66      0.64     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67     29888\n",
      "           1       0.68      0.61      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68     29888\n",
      "           1       0.69      0.60      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70     29888\n",
      "           1       0.71      0.56      0.63     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.67      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.81      0.70     29888\n",
      "           1       0.73      0.52      0.60     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.68      0.66      0.65     60000\n",
      "weighted avg       0.68      0.66      0.65     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.87      0.70     29888\n",
      "           1       0.75      0.40      0.52     30112\n",
      "\n",
      "    accuracy                           0.63     60000\n",
      "   macro avg       0.67      0.63      0.61     60000\n",
      "weighted avg       0.67      0.63      0.61     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.91      0.70     29888\n",
      "           1       0.77      0.32      0.45     30112\n",
      "\n",
      "    accuracy                           0.61     60000\n",
      "   macro avg       0.67      0.61      0.58     60000\n",
      "weighted avg       0.67      0.61      0.58     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.94      0.69     29888\n",
      "           1       0.80      0.22      0.34     30112\n",
      "\n",
      "    accuracy                           0.58     60000\n",
      "   macro avg       0.67      0.58      0.52     60000\n",
      "weighted avg       0.67      0.58      0.52     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.97      0.68     29888\n",
      "           1       0.83      0.12      0.22     30112\n",
      "\n",
      "    accuracy                           0.55     60000\n",
      "   macro avg       0.68      0.55      0.45     60000\n",
      "weighted avg       0.68      0.55      0.45     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.99      0.67     29888\n",
      "           1       0.86      0.03      0.06     30112\n",
      "\n",
      "    accuracy                           0.51     60000\n",
      "   macro avg       0.68      0.51      0.37     60000\n",
      "weighted avg       0.68      0.51      0.37     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kshag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kshag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     29888\n",
      "           1       0.00      0.00      0.00     30112\n",
      "\n",
      "    accuracy                           0.50     60000\n",
      "   macro avg       0.25      0.50      0.33     60000\n",
      "weighted avg       0.25      0.50      0.33     60000\n",
      "\n",
      "best th:  0.19999999999999973 best f1:  0.6624565997472943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tune threshold\n",
    "\n",
    "# use your selected threshold to calculate performance \n",
    "\n",
    "train_docs = list(train.text)\n",
    "train_labels = list(train.label)\n",
    "test_docs = list(test.text)\n",
    "test_labels = list(test.label)\n",
    "\n",
    "best_f1 = 0\n",
    "best_th = 0\n",
    "\n",
    "for th in np.arange(-1, 1.01, 0.1):\n",
    "    f1 = analyze_sentiment(train_docs, train_labels, th)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_th = th\n",
    "\n",
    "\n",
    "print(\"best th: \", best_th, \"best f1: \", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67     29888\n",
      "           1       0.68      0.61      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67     29888\n",
      "           1       0.68      0.61      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67     29888\n",
      "           1       0.68      0.61      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68     29888\n",
      "           1       0.68      0.61      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68     29888\n",
      "           1       0.68      0.61      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68     29888\n",
      "           1       0.68      0.61      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.66     60000\n",
      "weighted avg       0.66      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68     29888\n",
      "           1       0.69      0.60      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68     29888\n",
      "           1       0.69      0.60      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68     29888\n",
      "           1       0.69      0.60      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68     29888\n",
      "           1       0.69      0.60      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68     29888\n",
      "           1       0.69      0.60      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68     29888\n",
      "           1       0.69      0.59      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69     29888\n",
      "           1       0.69      0.59      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69     29888\n",
      "           1       0.70      0.59      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69     29888\n",
      "           1       0.70      0.58      0.64     30112\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69     29888\n",
      "           1       0.70      0.58      0.64     30112\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.66     60000\n",
      "weighted avg       0.67      0.67      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69     29888\n",
      "           1       0.70      0.58      0.63     30112\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.66     60000\n",
      "weighted avg       0.67      0.67      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69     29888\n",
      "           1       0.70      0.58      0.63     30112\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.66     60000\n",
      "weighted avg       0.67      0.67      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69     29888\n",
      "           1       0.71      0.57      0.63     30112\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.66     60000\n",
      "weighted avg       0.67      0.67      0.66     60000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69     29888\n",
      "           1       0.71      0.57      0.63     30112\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.66     60000\n",
      "weighted avg       0.67      0.67      0.66     60000\n",
      "\n",
      "best th:  0.2699999999999997 best f1:  0.6630400204452712\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for th in np.arange(best_th-.1, best_th+.1, 0.01):\n",
    "    f1=analyze_sentiment(train_docs, train_labels, th)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_th = th\n",
    "        \n",
    "print(\"best th: \", best_th, \"best f1: \", best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69      9968\n",
      "           1       0.69      0.58      0.63     10032\n",
      "\n",
      "    accuracy                           0.66     20000\n",
      "   macro avg       0.66      0.66      0.66     20000\n",
      "weighted avg       0.66      0.66      0.66     20000\n",
      "\n",
      "test f1:  0.6578496050005054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_f1 = analyze_sentiment(test_docs, test_labels, best_th)\n",
    "print(\"test f1: \", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Supervised Sentiment Analysis Using Word Vectors (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1: Train Word Vectors\n",
    "\n",
    "Write a function `train_wordvec(docs, vector_size)` as follows: (2 points)\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents\n",
    "    - `vector_size`: the dimension of word vectors\n",
    "- First tokenize `docs` into tokens, remove punctuations and only keep tokens with at least two characters\n",
    "- Use `gensim` package to train word vectors. Set the `vector size` and also carefully set other parameters such as `window`, `min_count` etc. Explain how you select these parameters. \n",
    "- return the trained word vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    okens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordvec(docs, vector_size = 100):\n",
    "    tokenized_docs = [tokenize(doc) for doc in docs]\n",
    "    wv_model = Word2Vec(\n",
    "        sentences=tokenized_docs,\n",
    "        vector_size=vector_size,\n",
    "        window=5,  \n",
    "        min_count=2,\n",
    "        workers=4, \n",
    "        sg=1  \n",
    "    )   \n",
    "    \n",
    "    return wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = train_wordvec(train[\"text\"], vector_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2: Generate Vector Representation for Documents (2.5 points)\n",
    "\n",
    "Write a function `generate_doc_vector(train_docs, test_docs, wv_model, wv_dim= 100, stop_words = None, min_df = 1)` as follows:\n",
    "- Take two inputs:\n",
    "    - `train_docs`: a list of train documents, \n",
    "    - `test_docs`: a list of train documents, \n",
    "    - `wv_model`: trained word vector model. \n",
    "    - `wv_dim`: dimensionality of word vector. Set the default value to 100.\n",
    "    - `stop_words`: whether to remove stopwords\n",
    "    - `min_df`: minimum document frequency\n",
    "- First vectorize each document using TFIDF vectorizer by considering stop_words and min_df configurations.\n",
    "- For each token in the vocabulary, look up for its word vector in `wv_model`. \n",
    "- Then the document vector (denoted as `d`) of `doc` can be calculated as the `TFIDF-weighted sum of the word vectors of its tokens`, i.e. $d = \\frac{1}{\\sum{tfidf_i}} * \\sum_{i \\in doc}{tfidf_i * v_i}$, where $v_i$ is the word vector of the i-th token, and $tfidf_i$ is the tfidf weigth of this token.\n",
    "- Return the vector representations of all `train_docs` as a numpy array of shape `(n, vector_size)`, where `n` is the number of documents in `train_docs` and `vector_size` is the dimension of word vectors. Create similar representations for `test_docs` (HINT: here you need to `transform` test documents so that they are on the same tfidf dimensions as train documents and then generate the vector representations).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc_vector(train_docs, test_docs, wv_model, wv_dim= 100,\n",
    "                        stop_words = None, min_df = 1):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=min_df)\n",
    "\n",
    "    tfidf_train = vectorizer.fit_transform(train_docs)\n",
    "    tfidf_test = vectorizer.transform(test_docs)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    train_vectors = []\n",
    "    test_vectors = []\n",
    "\n",
    "    def compute_vectors(tfidf_matrix, docs):\n",
    "        vectors = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            tfidf_row = tfidf_matrix[i]\n",
    "            feature_indices = tfidf_row.indices\n",
    "            tfidf_values = tfidf_row.data\n",
    "            weighted_sum = np.zeros(wv_dim)\n",
    "            weight_sum = 0\n",
    "            for j, idx in enumerate(feature_indices):\n",
    "                token = vocab[idx]\n",
    "                if token in wv_model.wv:\n",
    "                    weighted_sum += tfidf_values[j] * wv_model.wv[token]\n",
    "                    weight_sum += tfidf_values[j]\n",
    "\n",
    "            if weight_sum > 0:\n",
    "                vectors.append(weighted_sum / weight_sum)\n",
    "            else:\n",
    "                vectors.append(np.zeros(wv_dim))\n",
    "        return vectors\n",
    "\n",
    "    train_vec = compute_vectors(tfidf_train, train_docs)\n",
    "    test_vec = compute_vectors(tfidf_test, test_docs)\n",
    "    train_vec=np.array(train_vec)\n",
    "    test_vec = np.array(test_vec)\n",
    "   \n",
    "    return train_vec, test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X = generate_doc_vector(train[\"text\"], test[\"text\"], \n",
    "                                      wv_model, wv_dim= 100,\n",
    "                                      stop_words = None, min_df = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3: Put everything together (1 point)\n",
    "\n",
    "\n",
    "Define a function `predict_sentiment(train_text, train_label, test_text, test_label, wv_model, wv_dim= 100, stop_words = None, min_df = 1)` as follows:\n",
    "\n",
    "- Take the following inputs:\n",
    "    - `train_text, train_label`: a list of documents and their labels for training\n",
    "    - `test_text, test_label`: a list of documents and their labels for testing,\n",
    "    - `wv_model`: trained word vector model. \n",
    "    - `wv_dim`: dimensionality of word vector. Set the default value to 100.\n",
    "    - `stop_words`: whether to remove stopwords\n",
    "    - `min_df`: minimum document frequency\n",
    "- Call `generate_doc_vector` to generate vector representations (denoted as `train_X` and `test_X`) for documents in `train_text` and `test_text`. \n",
    "- Fit a linear SVM model using `train_X` and `train_label`\n",
    "- Predict the label for `test_X` and print out classification report for the testing subset.\n",
    "- This function has no return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(train_text, train_label, \n",
    "                      test_text, test_label, \n",
    "                      wv_model, wv_dim= 100,\n",
    "                      stop_words = None, min_df = 1):\n",
    "    train_X, test_X = generate_doc_vector(train_text, test_text, wv_model, wv_dim=wv_dim, stop_words=stop_words, min_df=min_df)\n",
    "\n",
    "    svm_model = LinearSVC()\n",
    "    svm_model.fit(train_X, train_label)\n",
    "\n",
    "    predicted_labels = svm_model.predict(test_X)\n",
    "\n",
    "    print(classification_report(test_label, predicted_labels))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4: Analysis (0.5 point)\n",
    "\n",
    "- Compare the classification reports you obtain from Q1 and Q2.3. Which model performs better?\n",
    "- Why this model can achieve better performance? Please explain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kshag\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72      9968\n",
      "           1       0.72      0.73      0.73     10032\n",
      "\n",
      "    accuracy                           0.72     20000\n",
      "   macro avg       0.72      0.72      0.72     20000\n",
      "weighted avg       0.72      0.72      0.72     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(train[\"text\"], train[\"label\"],\\\n",
    "                  test[\"text\"], test[\"label\"],\\\n",
    "                  wv_model, wv_dim= 100,\n",
    "                  stop_words = None, min_df = 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec-based model outperforms the Vader-based sentiment analysis from Q1, as we can see by the higher accuracy (.72 in Q2.3 compared to .66 in Q1) and f1 score (.72 in Q2.3 compared to .6578 in Q1) . The Word2Vec model achieves better performance due to its ability to better capture contextual word relationships, which provides a more detailed representation of the text data. Additionally, tf-idf weighting highlights important words while reducing the influence of common but not important words. The SVM classifier is also specifically optimized for sentiment classification, compared to the unsupervised Vader tool, which also helps to give us improved results in Q2.3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
